{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ASu-bT-Ks_8"
      },
      "source": [
        "# Connect Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcS4_BHmVINH",
        "outputId": "a19f63f7-dc8c-48bc-adc0-44c0e47963bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bt0W-VJkVjp3"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/drive/MyDrive/colab\n",
        "%cd /content/drive/MyDrive/colab\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCsq-HVgK0Xt"
      },
      "source": [
        "# ***1. Load Dataset - EDA***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NG97eFbSOMhT"
      },
      "source": [
        "Dataset\n",
        "- https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html (l·ªói bboxes)\n",
        "- https://datasets.activeloop.ai/docs/ml/datasets/celeba-dataset/\n",
        "- https://www.kaggle.com/datasets/jessicali9530/celeba-dataset  (l·ªói bboxes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jG0dGRyoB_-"
      },
      "outputs": [],
      "source": [
        "!pip install deeplake\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJC5wmI1n8v-"
      },
      "outputs": [],
      "source": [
        "import deeplake\n",
        "ds_train = deeplake.load(\"hub://activeloop/celeb-a-train\")[:10]\n",
        "\n",
        "img_list_train = ds_train['images'],\n",
        "anno_list_train = ds_train['boxes']\n",
        "print(img_list_train)\n",
        "print(anno_list_train)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# Xem qua m·ªôt s·ªë h√¨nh ·∫£nh t·ª´ t·∫≠p hu·∫•n luy·ªán\n",
        "i=0\n",
        "image = ds_train['images'][i]  # Tensor(key='images', index=Index([0]))\n",
        "print(image)\n",
        "# In h√¨nh d·∫°ng v√† ki·ªÉu d·ªØ li·ªáu c·ªßa ·∫£nh\n",
        "print(\"Shape of the image:\", image.shape) # Shape of the image: (295, 285, 3)\n",
        "print(\"Data type of the image:\", image.dtype) # Data type of the image: uint8\n",
        "\n",
        "image = ds_train['images'][i].numpy()  # Chuy·ªÉn tensor sang NumPy array\n",
        "print(image) # Numpy array\n",
        "print(\"Shape of the image:\", image.shape) # Shape of the image: (295, 285, 3)\n",
        "print(\"Data type of the image:\", image.dtype) # Data type of the image: uint8\n",
        "\n",
        "# Hi·ªÉn th·ªã ·∫£nh\n",
        "plt.imshow(image)\n",
        "plt.title(f\"Image {i}\")\n",
        "plt.show()\n",
        "\n",
        "# In th√¥ng tin c·∫•u tr√∫c c·ªßa bounding box\n",
        "box = ds_train['boxes'][i].numpy()  # L·∫•y th√¥ng tin bounding box v√† chuy·ªÉn sang NumPy array\n",
        "print(f\"Box shape for image {i}:\", box.shape)\n",
        "print(f\"First bounding box for image {i} (if multiple):\", box[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWqb_gIKUlYM"
      },
      "source": [
        "-\n",
        "```python\n",
        "image = ds_train['images'][i].numpy()  # Chuy·ªÉn tensor sang NumPy array\n",
        "box = ds_train['boxes'][i].numpy()  # L·∫•y th√¥ng tin bounding box v√† chuy·ªÉn sang NumPy array\n",
        "- bb d·∫°ng: x_min, y_min, width, height"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cn8ABE9Trppf"
      },
      "source": [
        "## Load Images and Bouding Boxes from deeplake to local colab\n",
        "Khi b·∫°n s·ª≠ d·ª•ng `deeplake.load` ƒë·ªÉ t·∫£i b·ªô d·ªØ li·ªáu t·ª´ Activeloop Hub, b·ªô d·ªØ li·ªáu ƒë∆∞·ª£c t·∫£i tr·ª±c ti·∫øp v√†o b·ªô nh·ªõ c·ªßa m√¥i tr∆∞·ªùng Python m√† b·∫°n ƒëang s·ª≠ d·ª•ng, nh∆∞ Google Colab ho·∫∑c Kaggle Kernel. N√≥ kh√¥ng ƒë∆∞·ª£c l∆∞u tr·ªØ tr√™n ·ªï c·ª©ng ho·∫∑c b·∫•t k·ª≥ thi·∫øt b·ªã l∆∞u tr·ªØ v·∫≠t l√Ω n√†o, tr·ª´ khi b·∫°n ch·ªß ƒë·ªông th·ª±c hi·ªán vi·ªác l∆∞u n√≥.\n",
        "\n",
        "D·ªØ li·ªáu trong `ds_train` sau khi ch·∫°y ƒëo·∫°n m√£ c·ªßa b·∫°n s·∫Ω t·ªìn t·∫°i d∆∞·ªõi d·∫°ng m·ªôt ƒë·ªëi t∆∞·ª£ng trong b·ªô nh·ªõ, c√≥ th·ªÉ truy c·∫≠p v√† x·ª≠ l√Ω trong phi√™n l√†m vi·ªác Python hi·ªán t·∫°i c·ªßa b·∫°n. Khi b·∫°n ng·∫Øt k·∫øt n·ªëi t·ª´ Colab ho·∫∑c Kaggle Kernel, ho·∫∑c khi phi√™n c·ªßa b·∫°n h·∫øt h·∫°n, d·ªØ li·ªáu n√†y s·∫Ω b·ªã m·∫•t tr·ª´ khi b·∫°n ƒë√£ l∆∞u n√≥ v√†o m·ªôt n∆°i l∆∞u tr·ªØ kh√°c nh∆∞ Google Drive ho·∫∑c Kaggle Datasets.\n",
        "\n",
        "N·∫øu b·∫°n mu·ªën l∆∞u m·ªôt ph·∫ßn ho·∫∑c to√†n b·ªô b·ªô d·ªØ li·ªáu n√†y v√†o ·ªï c·ª©ng ho·∫∑c m·ªôt d·ªãch v·ª• l∆∞u tr·ªØ ƒë√°m m√¢y, b·∫°n c·∫ßn vi·∫øt m√£ ƒë·ªÉ th·ª±c hi·ªán vi·ªác l∆∞u tr·ªØ ƒë√≥. ƒêi·ªÅu n√†y th∆∞·ªùng bao g·ªìm vi·ªác chuy·ªÉn d·ªØ li·ªáu v√†o ƒë·ªãnh d·∫°ng ph√π h·ª£p (v√≠ d·ª•, h√¨nh ·∫£nh th√†nh c√°c file ·∫£nh, d·ªØ li·ªáu tabular th√†nh CSV) v√† sau ƒë√≥ s·ª≠ d·ª•ng c√°c API ho·∫∑c th∆∞ vi·ªán ƒë·ªÉ l∆∞u n√≥ v√†o v·ªã tr√≠ mong mu·ªën."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMpA9WhpSwAQ"
      },
      "outputs": [],
      "source": [
        "!mkdir custom_dataset\n",
        "!mkdir custom_dataset/images_and_labels\n",
        "!mkdir custom_dataset/images_and_labels/images\n",
        "!mkdir custom_dataset/images_and_labels/labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwUsQQEmLMdw"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SiGHJ0F2SzKt"
      },
      "outputs": [],
      "source": [
        "import deeplake\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset using deeplake\n",
        "ds_train = deeplake.load(\"hub://activeloop/celeb-a-train\")[:10]\n",
        "\n",
        "# Create a directory to save images if it does not exist\n",
        "save_path = \"custom_dataset/images_and_labels/images/train\"\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "# Save images to the directory\n",
        "for i in range(len(ds_train['images'])):  # Assuming 'image' is the key for images\n",
        "    image_array = ds_train['images'][i].numpy()  # Convert to numpy array\n",
        "    if image_array.ndim == 3 and image_array.shape[2] == 3:  # Check if image is in correct format\n",
        "        image = Image.fromarray(image_array)\n",
        "        image_path = os.path.join(save_path, f'image_{i}.png')\n",
        "        image.save(image_path, \"PNG\")\n",
        "\n",
        "print(\"Images saved successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEeBMkY0ryjj"
      },
      "source": [
        "ƒê·ªÉ x·ª≠ l√Ω th√¥ng tin bounding box t·ª´ b·ªô d·ªØ li·ªáu CelebA v√† vi·∫øt ch√∫ng v√†o c√°c file `.txt` theo ƒë·ªãnh d·∫°ng c·ªßa YOLOv5, b·∫°n c·∫ßn th·ª±c hi·ªán c√°c b∆∞·ªõc sau:\n",
        "\n",
        "1. **X√°c ƒê·ªãnh T·ªça ƒê·ªô Bounding Box**: Trong YOLOv5, bounding box ƒë∆∞·ª£c bi·ªÉu di·ªÖn b·ªüi t·ªça ƒë·ªô x_center, y_center, width, v√† height. Tuy nhi√™n, t·ªça ƒë·ªô trong CelebA c√≥ th·ªÉ ƒë∆∞·ª£c bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng x_min, y_min, width, v√† height. Do ƒë√≥, b·∫°n c·∫ßn chuy·ªÉn ƒë·ªïi ch√∫ng.\n",
        "\n",
        "2. **Chuy·ªÉn ƒê·ªïi T·ªça ƒê·ªô**:\n",
        "   - x_center = x_min + (width / 2)\n",
        "   - y_center = y_min + (height / 2)\n",
        "\n",
        "3. **Normalize T·ªça ƒê·ªô**: Trong YOLOv5, t·ªça ƒë·ªô th∆∞·ªùng ƒë∆∞·ª£c chu·∫©n h√≥a theo k√≠ch th∆∞·ªõc c·ªßa ·∫£nh. ƒêi·ªÅu n√†y c√≥ nghƒ©a l√† x_center, y_center, width, v√† height ƒë·ªÅu l√† ph·∫ßn trƒÉm t∆∞∆°ng ƒë·ªëi c·ªßa k√≠ch th∆∞·ªõc ·∫£nh.\n",
        "   - x_center = x_center / chi·ªÅu r·ªông ·∫£nh\n",
        "   - y_center = y_center / chi·ªÅu cao ·∫£nh\n",
        "   - width = width / chi·ªÅu r·ªông ·∫£nh\n",
        "   - height = height / chi·ªÅu cao ·∫£nh\n",
        "\n",
        "4. **L∆∞u v√†o File `.txt`**: M·ªói ·∫£nh s·∫Ω c√≥ m·ªôt file `.txt` t∆∞∆°ng ·ª©ng v·ªõi n·ªôi dung l√† \"0\" v√† t·ªça ƒë·ªô bounding box ƒë√£ ƒë∆∞·ª£c chu·∫©n h√≥a v√† chuy·ªÉn ƒë·ªïi.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVx7MxaqS3UW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# T·∫°o th∆∞ m·ª•c ƒë·ªÉ l∆∞u file .txt n·∫øu ch∆∞a t·ªìn t·∫°i\n",
        "txt_save_path = \"custom_dataset/images_and_labels/labels/train\"\n",
        "os.makedirs(txt_save_path, exist_ok=True)\n",
        "\n",
        "for i in range(len(ds_train['images'])):\n",
        "    image = ds_train['images'][i].numpy()\n",
        "    box = ds_train['boxes'][i].numpy()[0]  # Gi·∫£ s·ª≠ ch·ªâ c√≥ m·ªôt bounding box cho m·ªói ·∫£nh\n",
        "\n",
        "    # Chuy·ªÉn ƒë·ªïi t·ªça ƒë·ªô\n",
        "    x_center = box[0] + (box[2] / 2)\n",
        "    y_center = box[1] + (box[3] / 2)\n",
        "\n",
        "    # Normalize t·ªça ƒë·ªô\n",
        "    x_center /= image.shape[1]  # Chia cho chi·ªÅu r·ªông ·∫£nh\n",
        "    y_center /= image.shape[0]  # Chia cho chi·ªÅu cao ·∫£nh\n",
        "    width_norm = box[2] / image.shape[1]\n",
        "    height_norm = box[3] / image.shape[0]\n",
        "\n",
        "    # T·∫°o n·ªôi dung cho file .txt\n",
        "    txt_content = f\"0 {x_center} {y_center} {width_norm} {height_norm}\"\n",
        "\n",
        "    # L∆∞u n·ªôi dung v√†o file .txt\n",
        "    txt_filename = f\"image_{i}.txt\"  # Ho·∫∑c s·ª≠ d·ª•ng t√™n ·∫£nh n·∫øu c√≥\n",
        "    with open(os.path.join(txt_save_path, txt_filename), 'w') as f:\n",
        "        f.write(txt_content)\n",
        "\n",
        "print(\"Bounding box annotations saved successfully.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXpztsuhtJgS"
      },
      "source": [
        "## Processing: Train - Val - Test for Yolov5s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zO7hHZR_OzWu"
      },
      "source": [
        "- https://github.com/deepakat002/yolov5_facemask\n",
        "- https://www.pandaml.com/train-yolov5/\n",
        "- https://www.kaggle.com/datasets/deepakat002/face-mask-detection-yolov5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7UBxY2vMXLI"
      },
      "source": [
        "```\n",
        "yolov5\n",
        "custom_dataset\n",
        "‚îú‚îÄ‚îÄ custom_dataset.yaml\n",
        "‚îú‚îÄ‚îÄ custom_model.yaml\n",
        "‚îî‚îÄ‚îÄ images_and_labels\n",
        "```\n",
        "\n",
        "```\n",
        "images_and_labels\n",
        "‚îú‚îÄ‚îÄ images\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ train\n",
        "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_001.jpg\n",
        "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_002.jpg\n",
        "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ valid\n",
        "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ valid_001.jpg\n",
        "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ valid_002.jpg\n",
        "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ test\n",
        "‚îÇ       ‚îú‚îÄ‚îÄ test_001.jpg\n",
        "‚îÇ       ‚îú‚îÄ‚îÄ test_002.jpg\n",
        "‚îÇ       ‚îî‚îÄ‚îÄ ...\n",
        "‚îî‚îÄ‚îÄ labels\n",
        "  ‚îú‚îÄ‚îÄ train\n",
        "  ‚îÇ   ‚îú‚îÄ‚îÄ train_001.txt\n",
        "  ‚îÇ   ‚îú‚îÄ‚îÄ train_002.txt\n",
        "  ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
        "  ‚îî‚îÄ‚îÄ valid\n",
        "      ‚îú‚îÄ‚îÄ valid_001.txt\n",
        "      ‚îú‚îÄ‚îÄ valid_002.txt\n",
        "      ‚îî‚îÄ‚îÄ ...\n",
        "```\n",
        "ho·∫∑c\n",
        "```\n",
        "images_and_labels\n",
        "‚îú‚îÄ‚îÄ train\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ images\n",
        "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_001.jpg\n",
        "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_002.jpg\n",
        "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
        "‚îÇ    ‚îÄ‚îÄ labels\n",
        "‚îÇ       ‚îú‚îÄ‚îÄ train_001.txt\n",
        "‚îÇ       ‚îú‚îÄ‚îÄ train_002.txt\n",
        "‚îÇ       ‚îî‚îÄ‚îÄ ...                \n",
        "‚îî‚îÄ‚îÄ val\n",
        "  ‚îú‚îÄ‚îÄ images\n",
        "  ‚îÇ   ‚îú‚îÄ‚îÄ valid_001.jpg\n",
        "  ‚îÇ   ‚îú‚îÄ‚îÄ valid_002.jpg\n",
        "  ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
        "  ‚îî‚îÄ‚îÄ labels\n",
        "      ‚îú‚îÄ‚îÄ valid_001.txt\n",
        "      ‚îú‚îÄ‚îÄ valid_002.txt\n",
        "      ‚îî‚îÄ‚îÄ ...\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcOmx2lKtJSw"
      },
      "outputs": [],
      "source": [
        "import deeplake\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "def process_dataset(ds, images_path, labels_path, dataset_type):\n",
        "    os.makedirs(images_path, exist_ok=True)\n",
        "    os.makedirs(labels_path, exist_ok=True)\n",
        "\n",
        "    for i, (image_array, box) in enumerate(zip(ds['images'], ds['boxes'])):\n",
        "        if image_array.ndim == 3 and image_array.shape[2] == 3:\n",
        "            # Save image\n",
        "            image = Image.fromarray(image_array.numpy())\n",
        "            image_path = os.path.join(images_path, f'image_{i}.png')\n",
        "            image.save(image_path, \"PNG\")\n",
        "\n",
        "            # Save bounding box\n",
        "            box = box.numpy()[0]\n",
        "            x_center, y_center = box[0] + box[2] / 2, box[1] + box[3] / 2\n",
        "            x_center /= image_array.shape[1]\n",
        "            y_center /= image_array.shape[0]\n",
        "            width_norm = box[2] / image_array.shape[1]\n",
        "            height_norm = box[3] / image_array.shape[0]\n",
        "            txt_content = f\"0 {x_center} {y_center} {width_norm} {height_norm}\"\n",
        "            txt_filename = f\"image_{i}.txt\"\n",
        "            with open(os.path.join(labels_path, txt_filename), 'w') as f:\n",
        "                f.write(txt_content)\n",
        "\n",
        "    print(f\"{dataset_type} images and annotations saved successfully.\")\n",
        "\n",
        "# Load datasets\n",
        "ds_train = deeplake.load(\"hub://activeloop/celeb-a-train\")[:1000]\n",
        "ds_val = deeplake.load(\"hub://activeloop/celeb-a-val\")[:100]\n",
        "ds_test = deeplake.load(\"hub://activeloop/celeb-a-test\")[:100]\n",
        "\n",
        "# Define paths\n",
        "base_path = \"custom_dataset/images_and_labels\"\n",
        "image_paths = {\n",
        "    \"train\": os.path.join(base_path, \"images/train\"),\n",
        "    \"valid\": os.path.join(base_path, \"images/valid\"),\n",
        "    \"test\": os.path.join(base_path, \"images/test\")\n",
        "}\n",
        "label_paths = {\n",
        "    \"train\": os.path.join(base_path, \"labels/train\"),\n",
        "    \"valid\": os.path.join(base_path, \"labels/valid\"),\n",
        "    \"test\": os.path.join(base_path, \"labels/test\")\n",
        "}\n",
        "\n",
        "# Process datasets\n",
        "process_dataset(ds_train, image_paths['train'], label_paths['train'], \"Train\")\n",
        "process_dataset(ds_val, image_paths['valid'], label_paths['valid'], \"Validation\")\n",
        "process_dataset(ds_test, image_paths['test'], label_paths['test'], \"Test\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fuK6NAvskrD"
      },
      "source": [
        "# ***2. Model: Yolov5s***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zn5WRnGyaUow"
      },
      "source": [
        "- https://www.youtube.com/watch?v=12UoOlsRwh8\n",
        "- https://github.com/deepakat002/yolov5_facemask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lk6RnTTFMbGG"
      },
      "source": [
        "Write custom_dataset.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVAVIpgPLi_4"
      },
      "outputs": [],
      "source": [
        "# !rm -r /kaggle/working/custom_dataset/custom_dataset.yaml\n",
        "\n",
        "# Create and write to a file (c√°ch t·∫°o file trong kaggle lu√¥n r√πi)\n",
        "with open('/content/drive/MyDrive/colab/custom_dataset/custom_dataset.yaml', 'w') as file:  # write ghi ƒë√®, a (append mode)\n",
        "    file.write(\"\"\"\n",
        "# train: /kaggle/working/custom_dataset/images_and_labels/images/train\n",
        "# val: /kaggle/working/custom_dataset/images_and_labels/images/valid\n",
        "train: /content/drive/MyDrive/colab/custom_dataset/images_and_labels/images/train\n",
        "val: /content/drive/MyDrive/colab/custom_dataset/images_and_labels/images/valid\n",
        "# number of classes\n",
        "nc: 1\n",
        "# class names\n",
        "names: ['face']\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5fg35suslbX"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/ultralytics/yolov5.git\n",
        "!cd yolov5 && pwd\n",
        "%cd yolov5\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlV9JHQusdfe"
      },
      "outputs": [],
      "source": [
        "# https://github.com/ultralytics/yolov5/releases/v6.0\n",
        "# https://github.com/ultralytics/yolov5/releases/v7.0\n",
        "!wget https://github.com/ultralytics/yolov5/releases/download/v6.0/yolov5s.pt\n",
        "# !wget https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZ-0dXuY48ug"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wJLt-cZEKXF",
        "outputId": "d7b97d65-4b82-4b5d-fa0d-5733f36fade3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.16.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.39.1-py2.py3-none-any.whl (254 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m254.1/254.1 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.11.17)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.40 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.39.1 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.1\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "id": "YUkm3bELCBwd",
        "outputId": "50abecba-5961-4ae6-dd86-a5164afffe14"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdoanngoccuong\u001b[0m (\u001b[33mdoanngoccuong_nh\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/colab/yolov5/wandb/run-20240102_224246-mqpjqdbm</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/doanngoccuong_nh/FaceDetection_Yolov5/runs/mqpjqdbm' target=\"_blank\">treasured-dream-6</a></strong> to <a href='https://wandb.ai/doanngoccuong_nh/FaceDetection_Yolov5' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/doanngoccuong_nh/FaceDetection_Yolov5' target=\"_blank\">https://wandb.ai/doanngoccuong_nh/FaceDetection_Yolov5</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/doanngoccuong_nh/FaceDetection_Yolov5/runs/mqpjqdbm' target=\"_blank\">https://wandb.ai/doanngoccuong_nh/FaceDetection_Yolov5/runs/mqpjqdbm</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/doanngoccuong_nh/FaceDetection_Yolov5/runs/mqpjqdbm?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7d04dc2ddcc0>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login(key = \"c8767797aae76cbcd389ff29929ace1ac3021161\")    # key's DoanNgocCuong\n",
        "wandb.init(\n",
        "    project = \"FaceDetection_Yolov5\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9RqksZuZSIe",
        "outputId": "e3afafe1-8bd4-4720-8894-c79f965ab648"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/colab/yolov5\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/colab/yolov5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zq1r7DDzIIX-"
      },
      "source": [
        "- Using a Pretrained Weight File\n",
        "The model's weights are adjusted during the training process on your custom dataset. This approach is often used for transfer learning, where the pretrained model has already learned features from a large and diverse dataset.\n",
        "- Using a Configuration File Without Pretrained Weights which defines the architecture of the model but does not use pretrained weights. The model will be trained from scratch on your custom dataset.\n",
        "\n",
        "- cfg: c·∫•u h√¨nh model t·ª´ ƒë·∫ßu,\n",
        "- weights ƒë·ªÉ tr·ªëng: ko x√†i pretrained model\n",
        "- cache: load data v√†o train.cache ·ªü RAM, t·ªëi ∆∞u vi·ªác train.\n",
        "- project (log with wandb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dvaKRpEInva",
        "outputId": "b9d9c81e-c458-47b9-e393-afcef9b1a11b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: WARNING ‚ö†Ô∏è wandb is deprecated and will be removed in a future release. See supported integrations at https://github.com/ultralytics/yolov5#integrations.\n",
            "2024-01-02 22:51:33.559906: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-02 22:51:33.559962: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-02 22:51:33.561554: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdoanngoccuong\u001b[0m (\u001b[33mdoanngoccuong_nh\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=, cfg=/content/drive/MyDrive/colab/yolov5/models/yolov5s.yaml, data=/content/drive/MyDrive/colab/custom_dataset/custom_dataset.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=10, batch_size=16, imgsz=416, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=Yolov5_FaceDetection, name=custom_model, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ‚úÖ\n",
            "YOLOv5 üöÄ v7.0-256-g43c43d8 Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 üöÄ runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir Yolov5_FaceDetection', view at http://localhost:6006/\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/colab/yolov5/wandb/run-20240102_225137-0zbu9kxu\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcustom_model\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/doanngoccuong_nh/Yolov5_FaceDetection\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/doanngoccuong_nh/Yolov5_FaceDetection/runs/0zbu9kxu\u001b[0m\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "YOLOv5s summary: 214 layers, 7022326 parameters, 7022326 gradients, 15.9 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/colab/custom_dataset/images_and_labels/labels/train.cache... 1000 images, 0 backgrounds, 0 corrupt: 100% 1000/1000 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.4GB ram): 100% 1000/1000 [00:21<00:00, 46.38it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/colab/custom_dataset/images_and_labels/labels/valid.cache... 100 images, 0 backgrounds, 0 corrupt: 100% 100/100 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB ram): 100% 100/100 [00:03<00:00, 25.00it/s]\n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m4.74 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ‚úÖ\n",
            "Plotting labels to Yolov5_FaceDetection/custom_model2/labels.jpg... \n",
            "Image sizes 416 train, 416 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mYolov5_FaceDetection/custom_model2\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        0/9      1.52G    0.09731    0.02315          0         20        416: 100% 63/63 [00:16<00:00,  3.81it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:01<00:00,  2.03it/s]\n",
            "                   all        100        100    0.00317       0.95    0.00539    0.00165\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        1/9      1.67G    0.08194    0.02555          0         16        416: 100% 63/63 [00:13<00:00,  4.78it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  4.06it/s]\n",
            "                   all        100        100    0.00277       0.83    0.00829    0.00201\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        2/9      1.67G    0.07684    0.02616          0         15        416: 100% 63/63 [00:12<00:00,  5.24it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:01<00:00,  3.57it/s]\n",
            "                   all        100        100     0.0392       0.04     0.0093    0.00204\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        3/9      1.67G    0.07366     0.0265          0         19        416: 100% 63/63 [00:09<00:00,  6.33it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:01<00:00,  3.80it/s]\n",
            "                   all        100        100    0.00243       0.73    0.00392   0.000934\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        4/9      1.67G    0.06928    0.02616          0         17        416: 100% 63/63 [00:11<00:00,  5.35it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.09it/s]\n",
            "                   all        100        100    0.00213       0.64    0.00213   0.000625\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        5/9      1.67G    0.06526    0.02616          0         17        416: 100% 63/63 [00:12<00:00,  4.98it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  4.86it/s]\n",
            "                   all        100        100      0.189       0.37       0.21     0.0594\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        6/9      1.67G    0.06044     0.0253          0         22        416: 100% 63/63 [00:11<00:00,  5.32it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:01<00:00,  3.49it/s]\n",
            "                   all        100        100      0.126       0.24      0.108     0.0423\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        7/9      1.67G    0.05698    0.02407          0         21        416: 100% 63/63 [00:09<00:00,  6.53it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:01<00:00,  3.72it/s]\n",
            "                   all        100        100      0.304       0.13     0.0865     0.0238\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        8/9      1.67G    0.05417    0.02423          0         20        416: 100% 63/63 [00:12<00:00,  5.15it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.24it/s]\n",
            "                   all        100        100      0.265       0.06     0.0244     0.0071\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        9/9      1.67G      0.053     0.0228          0         19        416: 100% 63/63 [00:13<00:00,  4.52it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:01<00:00,  3.10it/s]\n",
            "                   all        100        100      0.788      0.671      0.793      0.397\n",
            "\n",
            "10 epochs completed in 0.040 hours.\n",
            "Optimizer stripped from Yolov5_FaceDetection/custom_model2/weights/last.pt, 14.3MB\n",
            "Optimizer stripped from Yolov5_FaceDetection/custom_model2/weights/best.pt, 14.3MB\n",
            "\n",
            "Validating Yolov5_FaceDetection/custom_model2/weights/best.pt...\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:02<00:00,  1.86it/s]\n",
            "                   all        100        100      0.789      0.671      0.793      0.395\n",
            "Results saved to \u001b[1mYolov5_FaceDetection/custom_model2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      metrics/mAP_0.5 ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: metrics/mAP_0.5:0.95 ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    metrics/precision ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       metrics/recall ‚ñà‚ñá‚ñÅ‚ñÜ‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÜ‚ñÜ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train/box_loss ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train/cls_loss ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train/obj_loss ‚ñÇ‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñÉ‚ñÑ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val/box_loss ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñà‚ñÇ‚ñÇ‚ñÖ‚ñÜ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val/cls_loss ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val/obj_loss ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÑ‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÇ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                x/lr0 ‚ñà‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                x/lr1 ‚ñÇ‚ñÜ‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                x/lr2 ‚ñÇ‚ñÜ‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best/epoch 9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best/mAP_0.5 0.79292\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best/mAP_0.5:0.95 0.39734\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best/precision 0.78843\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          best/recall 0.67078\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      metrics/mAP_0.5 0.79264\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: metrics/mAP_0.5:0.95 0.39485\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    metrics/precision 0.78854\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       metrics/recall 0.67122\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train/box_loss 0.053\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train/cls_loss 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train/obj_loss 0.0228\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val/box_loss 0.03718\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val/cls_loss 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val/obj_loss 0.01163\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                x/lr0 0.00208\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                x/lr1 0.00208\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                x/lr2 0.00208\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mcustom_model\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/doanngoccuong_nh/Yolov5_FaceDetection/runs/0zbu9kxu\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Ô∏è‚ö° View job at \u001b[34m\u001b[4mhttps://wandb.ai/doanngoccuong_nh/Yolov5_FaceDetection/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEyNjkzMjYyNA==/version_details/v1\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 17 media file(s), 3 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240102_225137-0zbu9kxu/logs\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: WARNING ‚ö†Ô∏è wandb is deprecated and will be removed in a future release. See supported integrations at https://github.com/ultralytics/yolov5#integrations.\n"
          ]
        }
      ],
      "source": [
        "# Train t·ª´ scratch\n",
        "!python train.py --img 416 --batch 16 --epochs 10 \\\n",
        "  --data /content/drive/MyDrive/colab/custom_dataset/custom_dataset.yaml \\\n",
        "  --cfg /content/drive/MyDrive/colab/yolov5/models/yolov5s.yaml \\\n",
        "  --weights '' \\\n",
        "  --name facedet_celeba_cfgyolov5_colab --cache \\\n",
        "  --project Yolov5_FaceDetection\n",
        "\n",
        "\n",
        "# # Train t·ª´ pretrained model: !wget https://github.com/ultralytics/yolov5/releases/download/v6.0/yolov5s.pt\n",
        "# !python train.py --img 416 --batch 16 --epochs 10 \\\n",
        "#   --data /content/drive/MyDrive/colab/custom_dataset/custom_dataset.yaml \\\n",
        "#   --weights '/content/drive/MyDrive/colab/yolov5/yolov5s.pt' --name facedet_celeba_weightsyolov5s_colab --cache \\\n",
        "#   --project Yolov5_FaceDetection\n",
        "\n",
        "# # Train t·ª´ checkpoints model.\n",
        "# !python train.py --img 416 --batch 16 --epochs 10 \\\n",
        "#   --data /content/drive/MyDrive/colab/custom_dataset/custom_dataset.yaml \\\n",
        "#   --weights '/content/drive/MyDrive/colab/yolov5/Yolov5_FaceDetection/custom_model/weights/best.pt' --name checkpoint_model --cache \\\n",
        "#   --project Yolov5_FaceDetection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fcdsrTRrmql"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeU0xKL6r1ZV"
      },
      "source": [
        "## Inference or detection on new images\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9kJt97sSWn7"
      },
      "source": [
        "### Infer1: Train 1000, Val: 100, Test: 100. Use: pretrained model: yolov5s.pt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSai9nKFSWn7"
      },
      "source": [
        "\n",
        "```python\n",
        "# Train t·ª´ pretrained model: !wget https://github.com/ultralytics/yolov5/releases/download/v6.0/yolov5s.pt\n",
        "!python train.py --img 416 --batch 16 --epochs 10 \\\n",
        "  --data /content/drive/MyDrive/colab/custom_dataset/custom_dataset.yaml \\\n",
        "  --weights '/content/drive/MyDrive/colab/yolov5/yolov5s.pt' --name custom_model --cache \\\n",
        "  --project Yolov5_FaceDetection\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1HLkarqVatb"
      },
      "source": [
        "- Khi ko connect wandb, th√¨ sau train, th√¥ng tin train ƒë∆∞·ª£c l∆∞u t·∫°i.\n",
        "\n",
        "/content/drive/MyDrive/colab/yolov5/runs/train/custom_model123456\n",
        "- Nh∆∞ng khi connect wandb th√¨ ko th·∫•y runs/train n·ªØa, thay v√†o ƒë√≥ l√†:\n",
        "/content/drive/MyDrive/colab/yolov5/Yolov5_FaceDetection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oJJwwxhr9xD",
        "outputId": "65678972-e211-4ab5-a301-e20a988aad05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/content/drive/MyDrive/colab/yolov5/Yolov5_FaceDetection/custom_model/weights/best.pt'], source=/content/drive/MyDrive/colab/custom_dataset/images_and_labels/images/test, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_csv=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5 üöÄ v7.0-256-g43c43d8 Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/colab/yolov5/detect.py\", line 285, in <module>\n",
            "    main(opt)\n",
            "  File \"/content/drive/MyDrive/colab/yolov5/detect.py\", line 280, in main\n",
            "    run(**vars(opt))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/drive/MyDrive/colab/yolov5/detect.py\", line 101, in run\n",
            "    model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)\n",
            "  File \"/content/drive/MyDrive/colab/yolov5/models/common.py\", line 356, in __init__\n",
            "    model = attempt_load(weights if isinstance(weights, list) else w, device=device, inplace=True, fuse=fuse)\n",
            "  File \"/content/drive/MyDrive/colab/yolov5/models/experimental.py\", line 79, in attempt_load\n",
            "    ckpt = torch.load(attempt_download(w), map_location='cpu')  # load\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 986, in load\n",
            "    with _open_file_like(f, 'rb') as opened_file:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 435, in _open_file_like\n",
            "    return _open_file(name_or_buffer, mode)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 416, in __init__\n",
            "    super().__init__(open(name, mode))\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/colab/yolov5/Yolov5_FaceDetection/custom_model/weights/best.pt'\n"
          ]
        }
      ],
      "source": [
        "# --img 416 --save-txt --save-conf (ko c·∫ßn resize 416 c≈©ng ukiii)\n",
        "# Infer tr√™n 100 images test\n",
        "!python detect.py --source /content/drive/MyDrive/colab/custom_dataset/images_and_labels/images/test --weights /content/drive/MyDrive/colab/yolov5/Yolov5_FaceDetection/facedet_celeba_weightsyolov5s_colab/weights/best.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKAJkRKtbyil",
        "outputId": "bd3969ef-3de9-48fe-ef68-948b1161ed07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/content/drive/MyDrive/colab/yolov5/Yolov5_FaceDetection/facedet_celeba_weightsyolov5s_colab/weights/best.pt'], source=/content/drive/MyDrive/colab/custom_dataset/real_test, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_csv=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5 üöÄ v7.0-256-g43c43d8 Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/colab/yolov5/detect.py\", line 285, in <module>\n",
            "    main(opt)\n",
            "  File \"/content/drive/MyDrive/colab/yolov5/detect.py\", line 280, in main\n",
            "    run(**vars(opt))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/drive/MyDrive/colab/yolov5/detect.py\", line 101, in run\n",
            "    model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)\n",
            "  File \"/content/drive/MyDrive/colab/yolov5/models/common.py\", line 356, in __init__\n",
            "    model = attempt_load(weights if isinstance(weights, list) else w, device=device, inplace=True, fuse=fuse)\n",
            "  File \"/content/drive/MyDrive/colab/yolov5/models/experimental.py\", line 79, in attempt_load\n",
            "    ckpt = torch.load(attempt_download(w), map_location='cpu')  # load\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 1014, in load\n",
            "    return _load(opened_zipfile,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 1422, in _load\n",
            "    result = unpickler.load()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 1415, in find_class\n",
            "    return super().find_class(mod_name, name)\n",
            "ModuleNotFoundError: No module named 'dill'\n"
          ]
        }
      ],
      "source": [
        "# --img 416 --save-txt --save-conf (ko c·∫ßn resize 416 c≈©ng ukiii)\n",
        "!python detect.py --source /content/chidau.jpg --weights /content/drive/MyDrive/colab/yolov5/Yolov5_FaceDetection/facedet_celeba_weightsyolov5s_colab/weights/best.pt\n",
        "!python detect.py --source /content/PhanNgocLanDSC01419.JPG --weights /content/drive/MyDrive/colab/yolov5/Yolov5_FaceDetection/facedet_celeba_weightsyolov5s_colab/weights/best.pt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsbvfpLFSWn9"
      },
      "source": [
        "![chidau.jpg](attachment:chidau.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YH3uC-hGSWn9"
      },
      "source": [
        "![PhanNgocLanDSC01419.JPG](attachment:PhanNgocLanDSC01419.JPG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOFlef0uWjSa"
      },
      "source": [
        "### Infer2: Train 1000, Val: 100, Test: 100. Without pretrained model: yolov5s.pt,\n",
        "use config /content/drive/MyDrive/colab/yolov5/models/yolov5s.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrTx0vQeW5PW",
        "outputId": "cfdb8240-053c-42f3-c4ff-dea35fea2fc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/content/drive/MyDrive/colab/yolov5/Yolov5_FaceDetection/facedet_celeba_cfgyolov5_colab/weights/best.pt'], source=/content/drive/MyDrive/colab/custom_dataset/real_test, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_csv=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5 üöÄ v7.0-256-g43c43d8 Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
            "image 1/4 /content/drive/MyDrive/colab/custom_dataset/real_test/Long.jpg: 640x640 3 faces, 11.6ms\n",
            "image 2/4 /content/drive/MyDrive/colab/custom_dataset/real_test/PhanNgocLanDSC01419.JPG: 640x448 1 face, 53.5ms\n",
            "image 3/4 /content/drive/MyDrive/colab/custom_dataset/real_test/chidau.jpg: 640x640 2 faces, 11.7ms\n",
            "image 4/4 /content/drive/MyDrive/colab/custom_dataset/real_test/vinh.jpg: 384x640 1 face, 55.0ms\n",
            "Speed: 0.7ms pre-process, 33.0ms inference, 123.4ms NMS per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/exp10\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# --img 416 --save-txt --save-conf (ko c·∫ßn resize 416 c≈©ng ukiii)\n",
        "# # Infer tr√™n 100 images test\n",
        "# !python detect.py --source /content/drive/MyDrive/colab/custom_dataset/images_and_labels/images/test --weights /content/drive/MyDrive/colab/yolov5/Yolov5_FaceDetection/custom_model/weights/best.pt\n",
        "\n",
        "# Infer real_test\n",
        "!python detect.py --source /content/drive/MyDrive/colab/custom_dataset/real_test --weights /content/drive/MyDrive/colab/yolov5/Yolov5_FaceDetection/facedet_celeba_cfgyolov5_colab/weights/best.pt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xw3XxHARfVmF"
      },
      "source": [
        "- C∆° b·∫£n l√† ƒë·ªô ch√≠nh x√°c t·ªá h∆°n khi detect ·∫£nh Long th√¨ th√™m 2 c√°i √¥ vu√¥ng to t∆∞·ªõng.\n",
        "- Anh Phan Lan, ch·ªã H√† th√¨ b·ªã double boud nh∆∞ ki·ªÉu c√≥ 2 ng∆∞·ªùi. (do ·∫£nh g·ªëc c√≥ bounding box c·ªßa l·∫ßn infer tr∆∞·ªõc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExI26OkBfuis"
      },
      "source": [
        "mail BKE GNH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8RITk57elhG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
