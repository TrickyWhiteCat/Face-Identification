{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ASu-bT-Ks_8"
      },
      "source": [
        "# Connect Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcS4_BHmVINH",
        "outputId": "a19f63f7-dc8c-48bc-adc0-44c0e47963bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bt0W-VJkVjp3"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/drive/MyDrive/colab\n",
        "%cd /content/drive/MyDrive/colab\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCsq-HVgK0Xt"
      },
      "source": [
        "# ***1. Load Dataset - EDA***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NG97eFbSOMhT"
      },
      "source": [
        "Dataset\n",
        "- https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html (lỗi bboxes)\n",
        "- https://datasets.activeloop.ai/docs/ml/datasets/celeba-dataset/\n",
        "- https://www.kaggle.com/datasets/jessicali9530/celeba-dataset  (lỗi bboxes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jG0dGRyoB_-"
      },
      "outputs": [],
      "source": [
        "!pip install deeplake\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJC5wmI1n8v-"
      },
      "outputs": [],
      "source": [
        "import deeplake\n",
        "ds_train = deeplake.load(\"hub://activeloop/celeb-a-train\")[:10]\n",
        "\n",
        "img_list_train = ds_train['images'],\n",
        "anno_list_train = ds_train['boxes']\n",
        "print(img_list_train)\n",
        "print(anno_list_train)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# Xem qua một số hình ảnh từ tập huấn luyện\n",
        "i=0\n",
        "image = ds_train['images'][i]  # Tensor(key='images', index=Index([0]))\n",
        "print(image)\n",
        "# In hình dạng và kiểu dữ liệu của ảnh\n",
        "print(\"Shape of the image:\", image.shape) # Shape of the image: (295, 285, 3)\n",
        "print(\"Data type of the image:\", image.dtype) # Data type of the image: uint8\n",
        "\n",
        "image = ds_train['images'][i].numpy()  # Chuyển tensor sang NumPy array\n",
        "print(image) # Numpy array\n",
        "print(\"Shape of the image:\", image.shape) # Shape of the image: (295, 285, 3)\n",
        "print(\"Data type of the image:\", image.dtype) # Data type of the image: uint8\n",
        "\n",
        "# Hiển thị ảnh\n",
        "plt.imshow(image)\n",
        "plt.title(f\"Image {i}\")\n",
        "plt.show()\n",
        "\n",
        "# In thông tin cấu trúc của bounding box\n",
        "box = ds_train['boxes'][i].numpy()  # Lấy thông tin bounding box và chuyển sang NumPy array\n",
        "print(f\"Box shape for image {i}:\", box.shape)\n",
        "print(f\"First bounding box for image {i} (if multiple):\", box[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWqb_gIKUlYM"
      },
      "source": [
        "-\n",
        "```python\n",
        "image = ds_train['images'][i].numpy()  # Chuyển tensor sang NumPy array\n",
        "box = ds_train['boxes'][i].numpy()  # Lấy thông tin bounding box và chuyển sang NumPy array\n",
        "- bb dạng: x_min, y_min, width, height"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cn8ABE9Trppf"
      },
      "source": [
        "## Load Images and Bouding Boxes from deeplake to local colab\n",
        "Khi bạn sử dụng `deeplake.load` để tải bộ dữ liệu từ Activeloop Hub, bộ dữ liệu được tải trực tiếp vào bộ nhớ của môi trường Python mà bạn đang sử dụng, như Google Colab hoặc Kaggle Kernel. Nó không được lưu trữ trên ổ cứng hoặc bất kỳ thiết bị lưu trữ vật lý nào, trừ khi bạn chủ động thực hiện việc lưu nó.\n",
        "\n",
        "Dữ liệu trong `ds_train` sau khi chạy đoạn mã của bạn sẽ tồn tại dưới dạng một đối tượng trong bộ nhớ, có thể truy cập và xử lý trong phiên làm việc Python hiện tại của bạn. Khi bạn ngắt kết nối từ Colab hoặc Kaggle Kernel, hoặc khi phiên của bạn hết hạn, dữ liệu này sẽ bị mất trừ khi bạn đã lưu nó vào một nơi lưu trữ khác như Google Drive hoặc Kaggle Datasets.\n",
        "\n",
        "Nếu bạn muốn lưu một phần hoặc toàn bộ bộ dữ liệu này vào ổ cứng hoặc một dịch vụ lưu trữ đám mây, bạn cần viết mã để thực hiện việc lưu trữ đó. Điều này thường bao gồm việc chuyển dữ liệu vào định dạng phù hợp (ví dụ, hình ảnh thành các file ảnh, dữ liệu tabular thành CSV) và sau đó sử dụng các API hoặc thư viện để lưu nó vào vị trí mong muốn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMpA9WhpSwAQ"
      },
      "outputs": [],
      "source": [
        "!mkdir custom_dataset\n",
        "!mkdir custom_dataset/images_and_labels\n",
        "!mkdir custom_dataset/images_and_labels/images\n",
        "!mkdir custom_dataset/images_and_labels/labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwUsQQEmLMdw"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SiGHJ0F2SzKt"
      },
      "outputs": [],
      "source": [
        "import deeplake\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset using deeplake\n",
        "ds_train = deeplake.load(\"hub://activeloop/celeb-a-train\")[:10]\n",
        "\n",
        "# Create a directory to save images if it does not exist\n",
        "save_path = \"custom_dataset/images_and_labels/images/train\"\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "# Save images to the directory\n",
        "for i in range(len(ds_train['images'])):  # Assuming 'image' is the key for images\n",
        "    image_array = ds_train['images'][i].numpy()  # Convert to numpy array\n",
        "    if image_array.ndim == 3 and image_array.shape[2] == 3:  # Check if image is in correct format\n",
        "        image = Image.fromarray(image_array)\n",
        "        image_path = os.path.join(save_path, f'image_{i}.png')\n",
        "        image.save(image_path, \"PNG\")\n",
        "\n",
        "print(\"Images saved successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEeBMkY0ryjj"
      },
      "source": [
        "Để xử lý thông tin bounding box từ bộ dữ liệu CelebA và viết chúng vào các file `.txt` theo định dạng của YOLOv5, bạn cần thực hiện các bước sau:\n",
        "\n",
        "1. **Xác Định Tọa Độ Bounding Box**: Trong YOLOv5, bounding box được biểu diễn bởi tọa độ x_center, y_center, width, và height. Tuy nhiên, tọa độ trong CelebA có thể được biểu diễn dưới dạng x_min, y_min, width, và height. Do đó, bạn cần chuyển đổi chúng.\n",
        "\n",
        "2. **Chuyển Đổi Tọa Độ**:\n",
        "   - x_center = x_min + (width / 2)\n",
        "   - y_center = y_min + (height / 2)\n",
        "\n",
        "3. **Normalize Tọa Độ**: Trong YOLOv5, tọa độ thường được chuẩn hóa theo kích thước của ảnh. Điều này có nghĩa là x_center, y_center, width, và height đều là phần trăm tương đối của kích thước ảnh.\n",
        "   - x_center = x_center / chiều rộng ảnh\n",
        "   - y_center = y_center / chiều cao ảnh\n",
        "   - width = width / chiều rộng ảnh\n",
        "   - height = height / chiều cao ảnh\n",
        "\n",
        "4. **Lưu vào File `.txt`**: Mỗi ảnh sẽ có một file `.txt` tương ứng với nội dung là \"0\" và tọa độ bounding box đã được chuẩn hóa và chuyển đổi.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVx7MxaqS3UW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Tạo thư mục để lưu file .txt nếu chưa tồn tại\n",
        "txt_save_path = \"custom_dataset/images_and_labels/labels/train\"\n",
        "os.makedirs(txt_save_path, exist_ok=True)\n",
        "\n",
        "for i in range(len(ds_train['images'])):\n",
        "    image = ds_train['images'][i].numpy()\n",
        "    box = ds_train['boxes'][i].numpy()[0]  # Giả sử chỉ có một bounding box cho mỗi ảnh\n",
        "\n",
        "    # Chuyển đổi tọa độ\n",
        "    x_center = box[0] + (box[2] / 2)\n",
        "    y_center = box[1] + (box[3] / 2)\n",
        "\n",
        "    # Normalize tọa độ\n",
        "    x_center /= image.shape[1]  # Chia cho chiều rộng ảnh\n",
        "    y_center /= image.shape[0]  # Chia cho chiều cao ảnh\n",
        "    width_norm = box[2] / image.shape[1]\n",
        "    height_norm = box[3] / image.shape[0]\n",
        "\n",
        "    # Tạo nội dung cho file .txt\n",
        "    txt_content = f\"0 {x_center} {y_center} {width_norm} {height_norm}\"\n",
        "\n",
        "    # Lưu nội dung vào file .txt\n",
        "    txt_filename = f\"image_{i}.txt\"  # Hoặc sử dụng tên ảnh nếu có\n",
        "    with open(os.path.join(txt_save_path, txt_filename), 'w') as f:\n",
        "        f.write(txt_content)\n",
        "\n",
        "print(\"Bounding box annotations saved successfully.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXpztsuhtJgS"
      },
      "source": [
        "## Processing: Train - Val - Test for Yolov5s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zO7hHZR_OzWu"
      },
      "source": [
        "- https://github.com/deepakat002/yolov5_facemask\n",
        "- https://www.pandaml.com/train-yolov5/\n",
        "- https://www.kaggle.com/datasets/deepakat002/face-mask-detection-yolov5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7UBxY2vMXLI"
      },
      "source": [
        "```\n",
        "yolov5\n",
        "custom_dataset\n",
        "├── custom_dataset.yaml\n",
        "├── custom_model.yaml\n",
        "└── images_and_labels\n",
        "```\n",
        "\n",
        "```\n",
        "images_and_labels\n",
        "├── images\n",
        "│   ├── train\n",
        "│   │   ├── train_001.jpg\n",
        "│   │   ├── train_002.jpg\n",
        "│   │   └── ...\n",
        "│   ├── valid\n",
        "│   │   ├── valid_001.jpg\n",
        "│   │   ├── valid_002.jpg\n",
        "│   │   └── ...\n",
        "│   └── test\n",
        "│       ├── test_001.jpg\n",
        "│       ├── test_002.jpg\n",
        "│       └── ...\n",
        "└── labels\n",
        "  ├── train\n",
        "  │   ├── train_001.txt\n",
        "  │   ├── train_002.txt\n",
        "  │   └── ...\n",
        "  └── valid\n",
        "      ├── valid_001.txt\n",
        "      ├── valid_002.txt\n",
        "      └── ...\n",
        "```\n",
        "hoặc\n",
        "```\n",
        "images_and_labels\n",
        "├── train\n",
        "│   ├── images\n",
        "│   │   ├── train_001.jpg\n",
        "│   │   ├── train_002.jpg\n",
        "│   │   └── ...\n",
        "│    ── labels\n",
        "│       ├── train_001.txt\n",
        "│       ├── train_002.txt\n",
        "│       └── ...                \n",
        "└── val\n",
        "  ├── images\n",
        "  │   ├── valid_001.jpg\n",
        "  │   ├── valid_002.jpg\n",
        "  │   └── ...\n",
        "  └── labels\n",
        "      ├── valid_001.txt\n",
        "      ├── valid_002.txt\n",
        "      └── ...\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcOmx2lKtJSw"
      },
      "outputs": [],
      "source": [
        "import deeplake\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "def process_dataset(ds, images_path, labels_path, dataset_type):\n",
        "    os.makedirs(images_path, exist_ok=True)\n",
        "    os.makedirs(labels_path, exist_ok=True)\n",
        "\n",
        "    for i, (image_array, box) in enumerate(zip(ds['images'], ds['boxes'])):\n",
        "        if image_array.ndim == 3 and image_array.shape[2] == 3:\n",
        "            # Save image\n",
        "            image = Image.fromarray(image_array.numpy())\n",
        "            image_path = os.path.join(images_path, f'image_{i}.png')\n",
        "            image.save(image_path, \"PNG\")\n",
        "\n",
        "            # Save bounding box\n",
        "            box = box.numpy()[0]\n",
        "            x_center, y_center = box[0] + box[2] / 2, box[1] + box[3] / 2\n",
        "            x_center /= image_array.shape[1]\n",
        "            y_center /= image_array.shape[0]\n",
        "            width_norm = box[2] / image_array.shape[1]\n",
        "            height_norm = box[3] / image_array.shape[0]\n",
        "            txt_content = f\"0 {x_center} {y_center} {width_norm} {height_norm}\"\n",
        "            txt_filename = f\"image_{i}.txt\"\n",
        "            with open(os.path.join(labels_path, txt_filename), 'w') as f:\n",
        "                f.write(txt_content)\n",
        "\n",
        "    print(f\"{dataset_type} images and annotations saved successfully.\")\n",
        "\n",
        "# Load datasets\n",
        "ds_train = deeplake.load(\"hub://activeloop/celeb-a-train\")[:1000]\n",
        "ds_val = deeplake.load(\"hub://activeloop/celeb-a-val\")[:100]\n",
        "ds_test = deeplake.load(\"hub://activeloop/celeb-a-test\")[:100]\n",
        "\n",
        "# Define paths\n",
        "base_path = \"custom_dataset/images_and_labels\"\n",
        "image_paths = {\n",
        "    \"train\": os.path.join(base_path, \"images/train\"),\n",
        "    \"valid\": os.path.join(base_path, \"images/valid\"),\n",
        "    \"test\": os.path.join(base_path, \"images/test\")\n",
        "}\n",
        "label_paths = {\n",
        "    \"train\": os.path.join(base_path, \"labels/train\"),\n",
        "    \"valid\": os.path.join(base_path, \"labels/valid\"),\n",
        "    \"test\": os.path.join(base_path, \"labels/test\")\n",
        "}\n",
        "\n",
        "# Process datasets\n",
        "process_dataset(ds_train, image_paths['train'], label_paths['train'], \"Train\")\n",
        "process_dataset(ds_val, image_paths['valid'], label_paths['valid'], \"Validation\")\n",
        "process_dataset(ds_test, image_paths['test'], label_paths['test'], \"Test\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fuK6NAvskrD"
      },
      "source": [
        "# ***2. Model: Yolov5s***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zn5WRnGyaUow"
      },
      "source": [
        "- https://www.youtube.com/watch?v=12UoOlsRwh8\n",
        "- https://github.com/deepakat002/yolov5_facemask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lk6RnTTFMbGG"
      },
      "source": [
        "Write custom_dataset.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVAVIpgPLi_4"
      },
      "outputs": [],
      "source": [
        "# !rm -r /kaggle/working/custom_dataset/custom_dataset.yaml\n",
        "\n",
        "# Create and write to a file (cách tạo file trong kaggle luôn rùi)\n",
        "with open('/content/drive/MyDrive/colab/custom_dataset/custom_dataset.yaml', 'w') as file:  # write ghi đè, a (append mode)\n",
        "    file.write(\"\"\"\n",
        "# train: /kaggle/working/custom_dataset/images_and_labels/images/train\n",
        "# val: /kaggle/working/custom_dataset/images_and_labels/images/valid\n",
        "train: /content/drive/MyDrive/colab/custom_dataset/images_and_labels/images/train\n",
        "val: /content/drive/MyDrive/colab/custom_dataset/images_and_labels/images/valid\n",
        "# number of classes\n",
        "nc: 1\n",
        "# class names\n",
        "names: ['face']\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5fg35suslbX"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/ultralytics/yolov5.git\n",
        "!cd yolov5 && pwd\n",
        "%cd yolov5\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlV9JHQusdfe"
      },
      "outputs": [],
      "source": [
        "# https://github.com/ultralytics/yolov5/releases/v6.0\n",
        "# https://github.com/ultralytics/yolov5/releases/v7.0\n",
        "!wget https://github.com/ultralytics/yolov5/releases/download/v6.0/yolov5s.pt\n",
        "# !wget https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZ-0dXuY48ug"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wJLt-cZEKXF",
        "outputId": "d7b97d65-4b82-4b5d-fa0d-5733f36fade3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.16.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.39.1-py2.py3-none-any.whl (254 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.1/254.1 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.11.17)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.40 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.39.1 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.1\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "id": "YUkm3bELCBwd",
        "outputId": "50abecba-5961-4ae6-dd86-a5164afffe14"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdoanngoccuong\u001b[0m (\u001b[33mdoanngoccuong_nh\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/colab/yolov5/wandb/run-20240102_224246-mqpjqdbm</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/doanngoccuong_nh/FaceDetection_Yolov5/runs/mqpjqdbm' target=\"_blank\">treasured-dream-6</a></strong> to <a href='https://wandb.ai/doanngoccuong_nh/FaceDetection_Yolov5' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/doanngoccuong_nh/FaceDetection_Yolov5' target=\"_blank\">https://wandb.ai/doanngoccuong_nh/FaceDetection_Yolov5</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/doanngoccuong_nh/FaceDetection_Yolov5/runs/mqpjqdbm' target=\"_blank\">https://wandb.ai/doanngoccuong_nh/FaceDetection_Yolov5/runs/mqpjqdbm</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/doanngoccuong_nh/FaceDetection_Yolov5/runs/mqpjqdbm?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7d04dc2ddcc0>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login(key = \"c8767797aae76cbcd389ff29929ace1ac3021161\")    # key's DoanNgocCuong\n",
        "wandb.init(\n",
        "    project = \"FaceDetection_Yolov5\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9RqksZuZSIe",
        "outputId": "e3afafe1-8bd4-4720-8894-c79f965ab648"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/colab/yolov5\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/colab/yolov5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zq1r7DDzIIX-"
      },
      "source": [
        "- Using a Pretrained Weight File\n",
        "The model's weights are adjusted during the training process on your custom dataset. This approach is often used for transfer learning, where the pretrained model has already learned features from a large and diverse dataset.\n",
        "- Using a Configuration File Without Pretrained Weights which defines the architecture of the model but does not use pretrained weights. The model will be trained from scratch on your custom dataset.\n",
        "\n",
        "- cfg: cấu hình model từ đầu,\n",
        "- weights để trống: ko xài pretrained model\n",
        "- cache: load data vào train.cache ở RAM, tối ưu việc train.\n",
        "- project (log with wandb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dvaKRpEInva",
        "outputId": "b9d9c81e-c458-47b9-e393-afcef9b1a11b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: WARNING ⚠️ wandb is deprecated and will be removed in a future release. See supported integrations at https://github.com/ultralytics/yolov5#integrations.\n",
            "2024-01-02 22:51:33.559906: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-02 22:51:33.559962: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-02 22:51:33.561554: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdoanngoccuong\u001b[0m (\u001b[33mdoanngoccuong_nh\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=, cfg=/content/drive/MyDrive/colab/yolov5/models/yolov5s.yaml, data=/content/drive/MyDrive/colab/custom_dataset/custom_dataset.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=10, batch_size=16, imgsz=416, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=Yolov5_FaceDetection, name=custom_model, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n",
            "YOLOv5 🚀 v7.0-256-g43c43d8 Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir Yolov5_FaceDetection', view at http://localhost:6006/\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/colab/yolov5/wandb/run-20240102_225137-0zbu9kxu\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcustom_model\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/doanngoccuong_nh/Yolov5_FaceDetection\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/doanngoccuong_nh/Yolov5_FaceDetection/runs/0zbu9kxu\u001b[0m\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "YOLOv5s summary: 214 layers, 7022326 parameters, 7022326 gradients, 15.9 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/colab/custom_dataset/images_and_labels/labels/train.cache... 1000 images, 0 backgrounds, 0 corrupt: 100% 1000/1000 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.4GB ram): 100% 1000/1000 [00:21<00:00, 46.38it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/colab/custom_dataset/images_and_labels/labels/valid.cache... 100 images, 0 backgrounds, 0 corrupt: 100% 100/100 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB ram): 100% 100/100 [00:03<00:00, 25.00it/s]\n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m4.74 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\n",
            "Plotting labels to Yolov5_FaceDetection/custom_model2/labels.jpg... \n",
            "Image sizes 416 train, 416 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mYolov5_FaceDetection/custom_model2\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        0/9      1.52G    0.09731    0.02315          0         20        416: 100% 63/63 [00:16<00:00,  3.81it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:01<00:00,  2.03it/s]\n",
            "                   all        100        100    0.00317       0.95    0.00539    0.00165\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        1/9      1.67G    0.08194    0.02555          0         16        416: 100% 63/63 [00:13<00:00,  4.78it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  4.06it/s]\n",
            "                   all        100        100    0.00277       0.83    0.00829    0.00201\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        2/9      1.67G    0.07684    0.02616          0         15        416: 100% 63/63 [00:12<00:00,  5.24it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:01<00:00,  3.57it/s]\n",
            "                   all        100        100     0.0392       0.04     0.0093    0.00204\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        3/9      1.67G    0.07366     0.0265          0         19        416: 100% 63/63 [00:09<00:00,  6.33it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:01<00:00,  3.80it/s]\n",
            "                   all        100        100    0.00243       0.73    0.00392   0.000934\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        4/9      1.67G    0.06928    0.02616          0         17        416: 100% 63/63 [00:11<00:00,  5.35it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.09it/s]\n",
            "                   all        100        100    0.00213       0.64    0.00213   0.000625\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        5/9      1.67G    0.06526    0.02616          0         17        416: 100% 63/63 [00:12<00:00,  4.98it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  4.86it/s]\n",
            "                   all        100        100      0.189       0.37       0.21     0.0594\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        6/9      1.67G    0.06044     0.0253          0         22        416: 100% 63/63 [00:11<00:00,  5.32it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:01<00:00,  3.49it/s]\n",
            "                   all        100        100      0.126       0.24      0.108     0.0423\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        7/9      1.67G    0.05698    0.02407          0         21        416: 100% 63/63 [00:09<00:00,  6.53it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:01<00:00,  3.72it/s]\n",
            "                   all        100        100      0.304       0.13     0.0865     0.0238\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        8/9      1.67G    0.05417    0.02423          0         20        416: 100% 63/63 [00:12<00:00,  5.15it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:00<00:00,  5.24it/s]\n",
            "                   all        100        100      0.265       0.06     0.0244     0.0071\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        9/9      1.67G      0.053     0.0228          0         19        416: 100% 63/63 [00:13<00:00,  4.52it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:01<00:00,  3.10it/s]\n",
            "                   all        100        100      0.788      0.671      0.793      0.397\n",
            "\n",
            "10 epochs completed in 0.040 hours.\n",
            "Optimizer stripped from Yolov5_FaceDetection/custom_model2/weights/last.pt, 14.3MB\n",
            "Optimizer stripped from Yolov5_FaceDetection/custom_model2/weights/best.pt, 14.3MB\n",
            "\n",
            "Validating Yolov5_FaceDetection/custom_model2/weights/best.pt...\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:02<00:00,  1.86it/s]\n",
            "                   all        100        100      0.789      0.671      0.793      0.395\n",
            "Results saved to \u001b[1mYolov5_FaceDetection/custom_model2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      metrics/mAP_0.5 ▁▁▁▁▁▃▂▂▁██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: metrics/mAP_0.5:0.95 ▁▁▁▁▁▂▂▁▁██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    metrics/precision ▁▁▁▁▁▃▂▄▃██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       metrics/recall █▇▁▆▆▄▃▂▁▆▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train/box_loss █▆▅▄▄▃▂▂▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train/cls_loss ▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train/obj_loss ▂▆▇█▇▇▆▃▄▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val/box_loss ▆▅▅▆█▂▂▅▆▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val/cls_loss ▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val/obj_loss ▄▄▃▃▁▄█▁▁▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                x/lr0 █▅▂▂▂▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                x/lr1 ▂▆█▇▇▆▅▃▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                x/lr2 ▂▆█▇▇▆▅▃▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           best/epoch 9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best/mAP_0.5 0.79292\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best/mAP_0.5:0.95 0.39734\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best/precision 0.78843\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          best/recall 0.67078\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      metrics/mAP_0.5 0.79264\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: metrics/mAP_0.5:0.95 0.39485\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    metrics/precision 0.78854\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       metrics/recall 0.67122\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train/box_loss 0.053\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train/cls_loss 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train/obj_loss 0.0228\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val/box_loss 0.03718\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val/cls_loss 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val/obj_loss 0.01163\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                x/lr0 0.00208\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                x/lr1 0.00208\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                x/lr2 0.00208\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mcustom_model\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/doanngoccuong_nh/Yolov5_FaceDetection/runs/0zbu9kxu\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ️⚡ View job at \u001b[34m\u001b[4mhttps://wandb.ai/doanngoccuong_nh/Yolov5_FaceDetection/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEyNjkzMjYyNA==/version_details/v1\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 17 media file(s), 3 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240102_225137-0zbu9kxu/logs\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: WARNING ⚠️ wandb is deprecated and will be removed in a future release. See supported integrations at https://github.com/ultralytics/yolov5#integrations.\n"
          ]
        }
      ],
      "source": [
        "# Train từ scratch\n",
        "!python train.py --img 416 --batch 16 --epochs 10 \\\n",
        "  --data /content/drive/MyDrive/colab/custom_dataset/custom_dataset.yaml \\\n",
        "  --cfg /content/drive/MyDrive/colab/yolov5/models/yolov5s.yaml \\\n",
        "  --weights '' \\\n",
        "  --name facedet_celeba_cfgyolov5_colab --cache \\\n",
        "  --project Yolov5_FaceDetection\n",
        "\n",
        "\n",
        "# # Train từ pretrained model: !wget https://github.com/ultralytics/yolov5/releases/download/v6.0/yolov5s.pt\n",
        "# !python train.py --img 416 --batch 16 --epochs 10 \\\n",
        "#   --data /content/drive/MyDrive/colab/custom_dataset/custom_dataset.yaml \\\n",
        "#   --weights '/content/drive/MyDrive/colab/yolov5/yolov5s.pt' --name facedet_celeba_weightsyolov5s_colab --cache \\\n",
        "#   --project Yolov5_FaceDetection\n",
        "\n",
        "# # Train từ checkpoints model.\n",
        "# !python train.py --img 416 --batch 16 --epochs 10 \\\n",
        "#   --data /content/drive/MyDrive/colab/custom_dataset/custom_dataset.yaml \\\n",
        "#   --weights '/content/drive/MyDrive/colab/yolov5/Yolov5_FaceDetection/custom_model/weights/best.pt' --name checkpoint_model --cache \\\n",
        "#   --project Yolov5_FaceDetection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fcdsrTRrmql"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeU0xKL6r1ZV"
      },
      "source": [
        "## Inference or detection on new images\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9kJt97sSWn7"
      },
      "source": [
        "### Infer1: Train 1000, Val: 100, Test: 100. Use: pretrained model: yolov5s.pt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSai9nKFSWn7"
      },
      "source": [
        "\n",
        "```python\n",
        "# Train từ pretrained model: !wget https://github.com/ultralytics/yolov5/releases/download/v6.0/yolov5s.pt\n",
        "!python train.py --img 416 --batch 16 --epochs 10 \\\n",
        "  --data /content/drive/MyDrive/colab/custom_dataset/custom_dataset.yaml \\\n",
        "  --weights '/content/drive/MyDrive/colab/yolov5/yolov5s.pt' --name custom_model --cache \\\n",
        "  --project Yolov5_FaceDetection\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1HLkarqVatb"
      },
      "source": [
        "- Khi ko connect wandb, thì sau train, thông tin train được lưu tại.\n",
        "\n",
        "/content/drive/MyDrive/colab/yolov5/runs/train/custom_model123456\n",
        "- Nhưng khi connect wandb thì ko thấy runs/train nữa, thay vào đó là:\n",
        "/content/drive/MyDrive/colab/yolov5/Yolov5_FaceDetection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oJJwwxhr9xD",
        "outputId": "65678972-e211-4ab5-a301-e20a988aad05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/content/drive/MyDrive/colab/yolov5/Yolov5_FaceDetection/custom_model/weights/best.pt'], source=/content/drive/MyDrive/colab/custom_dataset/images_and_labels/images/test, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_csv=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5 🚀 v7.0-256-g43c43d8 Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/colab/yolov5/detect.py\", line 285, in <module>\n",
            "    main(opt)\n",
            "  File \"/content/drive/MyDrive/colab/yolov5/detect.py\", line 280, in main\n",
            "    run(**vars(opt))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/drive/MyDrive/colab/yolov5/detect.py\", line 101, in run\n",
            "    model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)\n",
            "  File \"/content/drive/MyDrive/colab/yolov5/models/common.py\", line 356, in __init__\n",
            "    model = attempt_load(weights if isinstance(weights, list) else w, device=device, inplace=True, fuse=fuse)\n",
            "  File \"/content/drive/MyDrive/colab/yolov5/models/experimental.py\", line 79, in attempt_load\n",
            "    ckpt = torch.load(attempt_download(w), map_location='cpu')  # load\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 986, in load\n",
            "    with _open_file_like(f, 'rb') as opened_file:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 435, in _open_file_like\n",
            "    return _open_file(name_or_buffer, mode)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 416, in __init__\n",
            "    super().__init__(open(name, mode))\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/colab/yolov5/Yolov5_FaceDetection/custom_model/weights/best.pt'\n"
          ]
        }
      ],
      "source": [
        "# --img 416 --save-txt --save-conf (ko cần resize 416 cũng ukiii)\n",
        "# Infer trên 100 images test\n",
        "!python detect.py --source /content/drive/MyDrive/colab/custom_dataset/images_and_labels/images/test --weights /content/drive/MyDrive/colab/yolov5/Yolov5_FaceDetection/facedet_celeba_weightsyolov5s_colab/weights/best.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKAJkRKtbyil",
        "outputId": "bd3969ef-3de9-48fe-ef68-948b1161ed07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/content/drive/MyDrive/colab/yolov5/Yolov5_FaceDetection/facedet_celeba_weightsyolov5s_colab/weights/best.pt'], source=/content/drive/MyDrive/colab/custom_dataset/real_test, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_csv=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5 🚀 v7.0-256-g43c43d8 Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/colab/yolov5/detect.py\", line 285, in <module>\n",
            "    main(opt)\n",
            "  File \"/content/drive/MyDrive/colab/yolov5/detect.py\", line 280, in main\n",
            "    run(**vars(opt))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/drive/MyDrive/colab/yolov5/detect.py\", line 101, in run\n",
            "    model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)\n",
            "  File \"/content/drive/MyDrive/colab/yolov5/models/common.py\", line 356, in __init__\n",
            "    model = attempt_load(weights if isinstance(weights, list) else w, device=device, inplace=True, fuse=fuse)\n",
            "  File \"/content/drive/MyDrive/colab/yolov5/models/experimental.py\", line 79, in attempt_load\n",
            "    ckpt = torch.load(attempt_download(w), map_location='cpu')  # load\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 1014, in load\n",
            "    return _load(opened_zipfile,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 1422, in _load\n",
            "    result = unpickler.load()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 1415, in find_class\n",
            "    return super().find_class(mod_name, name)\n",
            "ModuleNotFoundError: No module named 'dill'\n"
          ]
        }
      ],
      "source": [
        "# --img 416 --save-txt --save-conf (ko cần resize 416 cũng ukiii)\n",
        "!python detect.py --source /content/chidau.jpg --weights /content/drive/MyDrive/colab/yolov5/Yolov5_FaceDetection/facedet_celeba_weightsyolov5s_colab/weights/best.pt\n",
        "!python detect.py --source /content/PhanNgocLanDSC01419.JPG --weights /content/drive/MyDrive/colab/yolov5/Yolov5_FaceDetection/facedet_celeba_weightsyolov5s_colab/weights/best.pt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsbvfpLFSWn9"
      },
      "source": [
        "![chidau.jpg](attachment:chidau.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YH3uC-hGSWn9"
      },
      "source": [
        "![PhanNgocLanDSC01419.JPG](attachment:PhanNgocLanDSC01419.JPG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOFlef0uWjSa"
      },
      "source": [
        "### Infer2: Train 1000, Val: 100, Test: 100. Without pretrained model: yolov5s.pt,\n",
        "use config /content/drive/MyDrive/colab/yolov5/models/yolov5s.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrTx0vQeW5PW",
        "outputId": "cfdb8240-053c-42f3-c4ff-dea35fea2fc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/content/drive/MyDrive/colab/yolov5/Yolov5_FaceDetection/facedet_celeba_cfgyolov5_colab/weights/best.pt'], source=/content/drive/MyDrive/colab/custom_dataset/real_test, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_csv=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5 🚀 v7.0-256-g43c43d8 Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
            "image 1/4 /content/drive/MyDrive/colab/custom_dataset/real_test/Long.jpg: 640x640 3 faces, 11.6ms\n",
            "image 2/4 /content/drive/MyDrive/colab/custom_dataset/real_test/PhanNgocLanDSC01419.JPG: 640x448 1 face, 53.5ms\n",
            "image 3/4 /content/drive/MyDrive/colab/custom_dataset/real_test/chidau.jpg: 640x640 2 faces, 11.7ms\n",
            "image 4/4 /content/drive/MyDrive/colab/custom_dataset/real_test/vinh.jpg: 384x640 1 face, 55.0ms\n",
            "Speed: 0.7ms pre-process, 33.0ms inference, 123.4ms NMS per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/exp10\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# --img 416 --save-txt --save-conf (ko cần resize 416 cũng ukiii)\n",
        "# # Infer trên 100 images test\n",
        "# !python detect.py --source /content/drive/MyDrive/colab/custom_dataset/images_and_labels/images/test --weights /content/drive/MyDrive/colab/yolov5/Yolov5_FaceDetection/custom_model/weights/best.pt\n",
        "\n",
        "# Infer real_test\n",
        "!python detect.py --source /content/drive/MyDrive/colab/custom_dataset/real_test --weights /content/drive/MyDrive/colab/yolov5/Yolov5_FaceDetection/facedet_celeba_cfgyolov5_colab/weights/best.pt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xw3XxHARfVmF"
      },
      "source": [
        "- Cơ bản là độ chính xác tệ hơn khi detect ảnh Long thì thêm 2 cái ô vuông to tướng.\n",
        "- Anh Phan Lan, chị Hà thì bị double boud như kiểu có 2 người. (do ảnh gốc có bounding box của lần infer trước)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExI26OkBfuis"
      },
      "source": [
        "mail BKE GNH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8RITk57elhG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
