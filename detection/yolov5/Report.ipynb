{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***1. INTRODUCTION***\n",
    "\\section{Introduction}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***2. DATA PROCESSING***\n",
    "\\section{Data processing}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***3. PRELIMINARIES***\n",
    "\\section{Preliminaries}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Face Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tóm tắt đoạn LaTeX bạn đã cung cấp:\n",
    "\n",
    "1. **Phát hiện Khuôn Mặt**: Đoạn này không được mô tả chi tiết.\n",
    "\n",
    "2. **Xác minh Khuôn Mặt**: Mục tiêu chính là xác định liệu hai hình ảnh có thể hiện cùng một người hay không. Phương pháp tiếp cận bao gồm việc sử dụng một hàm mã hóa để tạo ra các vector nhúng từ ảnh, giúp so sánh hiệu quả hơn.\n",
    "\n",
    "3. **Mã Hóa Khuôn Mặt**: Sử dụng kiến trúc mạng nơ-ron tiên tiến để xử lý hình ảnh khuôn mặt RGB, tạo ra các vector nhúng 128 chiều chứa các đặc trưng nổi bật.\n",
    "\n",
    "4. **Mất Mát Ba Phần Tử (Triplet Loss)**: Đào tạo mô hình với ba loại hình ảnh: Ảnh Neo (Anchor), Ảnh Dương (Positive) và Ảnh Âm (Negative). Mục tiêu là làm cho mã hóa của Ảnh Neo gần với Ảnh Dương hơn so với Ảnh Âm.\n",
    "\n",
    "5. **Mất Mát Ba Phần Tử Trực Tuyến (Online Triplet Loss)**: Trong phiên bản này, các bộ ba được chọn động trong mỗi lần lặp đào tạo. Một phương pháp chọn mẫu trong DataLoader được sử dụng để đảm bảo số lượng bộ ba cụ thể trong mỗi lô.\n",
    "\n",
    "6. **Mất Mát Biên Góc Cộng (Additive Angular Margin Loss)**: Mục tiêu là cải thiện sức mạnh phân biệt của mô hình nhận dạng khuôn mặt bằng cách thêm hình phạt vào góc giữa đặc trưng hiện tại và trọng số mục tiêu.\n",
    "\n",
    "7. **Tích hợp Mô Hình**: Bao gồm ba bước - Phát hiện và căn chỉnh khuôn mặt, Trích xuất khuôn mặt (Tạo vector nhúng), và Phân loại khuôn mặt (Xác minh khuôn mặt).\n",
    "\n",
    "Đây là một bản tóm tắt ngắn gọn về các khía cạnh kỹ thuật và lý thuyết liên quan đến xác minh khuôn mặt và các phương pháp liên quan được sử dụng trong xử lý ảnh kỹ thuật số và nhận dạng khuôn mặt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Preliminaries\" section of your Face Detection documentation is well-structured and comprehensively covers key aspects of face detection and verification technologies. Here are some observations and suggestions:\n",
    "Phần \"Sơ bộ\" trong tài liệu Phát hiện khuôn mặt của bạn có cấu trúc tốt và bao gồm toàn diện các khía cạnh chính của công nghệ phát hiện và xác minh khuôn mặt. Dưới đây là một số quan sát và gợi ý:\n",
    "\n",
    "Neural Network Architectures: This subsection effectively introduces the role of Convolutional Neural Networks in face detection. It's clear and concise, providing a good foundation for readers unfamiliar with CNNs.\n",
    "Kiến trúc mạng nơ-ron: Tiểu mục này giới thiệu hiệu quả vai trò của Mạng nơ-ron tích chập trong việc phát hiện khuôn mặt. Nó rõ ràng và súc tích, cung cấp một nền tảng tốt cho độc giả không quen thuộc với CNN.\n",
    "\n",
    "Image Processing and Bounding Box Prediction: You clearly describe the process of segmenting images into grids and predicting bounding boxes. This helps in understanding how face detection models localize faces within images.\n",
    "Xử lý hình ảnh và dự đoán hộp giới hạn: Bạn mô tả rõ ràng quá trình phân đoạn hình ảnh thành lưới và dự đoán các hộp giới hạn. Điều này giúp hiểu cách các mô hình phát hiện khuôn mặt định vị khuôn mặt trong hình ảnh.\n",
    "\n",
    "Augmentation: This subsection emphasizes the importance of diverse training datasets for effective face recognition. Mentioning specific data augmentation techniques is helpful for understanding how models are trained to generalize across various conditions.\n",
    "Tăng cường: Tiểu mục này nhấn mạnh tầm quan trọng của các bộ dữ liệu đào tạo đa dạng để nhận dạng khuôn mặt hiệu quả. Đề cập đến các kỹ thuật tăng cường dữ liệu cụ thể rất hữu ích để hiểu cách các mô hình được đào tạo để khái quát hóa trong các điều kiện khác nhau.\n",
    "\n",
    "Performance Evaluation Metrics: The explanation of metrics like accuracy, precision, recall, and Intersection Over Union (IOU) is insightful. These are crucial for understanding how the performance of face detection models is assessed.\n",
    "Chỉ số đánh giá hiệu suất: Giải thích về các số liệu như độ chính xác, độ chính xác, thu hồi và Giao lộ qua Liên minh (IOU) rất sâu sắc. Đây là những điều rất quan trọng để hiểu cách đánh giá hiệu suất của các mô hình phát hiện khuôn mặt.\n",
    "\n",
    "Face Verification: Introducing face verification as distinct from detection is important. The explanation of face encoding and the use of embedding vectors is clear and informative.\n",
    "Xác minh khuôn mặt: Giới thiệu xác minh khuôn mặt khác với phát hiện là rất quan trọng. Giải thích về mã hóa khuôn mặt và việc sử dụng các vectơ nhúng là rõ ràng và nhiều thông tin.\n",
    "\n",
    "Triplet Loss and Online Triplet Loss: These sections are particularly well-written, offering a detailed insight into the training processes of face verification models. The explanation of hard and semi-hard triplets is a nice touch.\n",
    "Mất ba và mất ba trực tuyến: Các phần này được viết đặc biệt tốt, cung cấp cái nhìn sâu sắc chi tiết về các quy trình đào tạo của các mô hình xác minh khuôn mặt. Lời giải thích về bộ ba cứng và bán cứng là một liên lạc tốt đẹp.\n",
    "\n",
    "Additive Angular Margin Loss: This section adds depth to the understanding of how face verification models are fine-tuned for accuracy and stability.\n",
    "Mất lề góc cộng thêm: Phần này bổ sung thêm chiều sâu cho sự hiểu biết về cách các mô hình xác minh khuôn mặt được tinh chỉnh để có độ chính xác và ổn định.\n",
    "\n",
    "Visual Aids: Adding diagrams or figures could significantly enhance the comprehension of the concepts discussed. For example, visual representations of CNN architectures, bounding box predictions, and the triplet loss concept could be very beneficial.\n",
    "Hỗ trợ trực quan: Thêm sơ đồ hoặc số liệu có thể nâng cao đáng kể sự hiểu biết về các khái niệm được thảo luận. Ví dụ, biểu diễn trực quan của kiến trúc CNN, dự đoán hộp giới hạn và khái niệm mất ba lần có thể rất có lợi.\n",
    "\n",
    "Suggestions for Improvement: Đề xuất cải tiến: \n",
    "\n",
    "Consider adding a brief introduction at the beginning to set the context for the reader.\n",
    "Cân nhắc thêm phần giới thiệu ngắn gọn ở đầu để đặt ngữ cảnh cho người đọc.\n",
    "A glossary of terms might be helpful, given the technical nature of the content.\n",
    "Một bảng thuật ngữ thuật ngữ có thể hữu ích, do tính chất kỹ thuật của nội dung.\n",
    "Including case studies or real-world applications of these technologies could provide practical insights.\n",
    "Bao gồm các nghiên cứu điển hình hoặc các ứng dụng trong thế giới thực của các công nghệ này có thể cung cấp những hiểu biết thực tế.\n",
    "Overall, the content is well-written and informative, suitable for readers with some background in machine learning and computer vision. Adding visual aids and a few enhancements as suggested would make it even more accessible and engaging.\n",
    "Nhìn chung, nội dung được viết tốt và nhiều thông tin, phù hợp với độc giả có một số nền tảng về học máy và thị giác máy tính. Thêm các phương tiện trực quan và một vài cải tiến như được đề xuất sẽ làm cho nó thậm chí còn dễ tiếp cận và hấp dẫn hơn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Preliminaries\" section of your report provides a detailed and comprehensive overview of Face Detection and Face Verification technologies. Here are some specific observations and recommendations:\n",
    "\n",
    "### Face Detection\n",
    "\n",
    "1. **Neural Network Architectures**: This part effectively introduces the importance of CNNs in face detection. The explanation of how CNNs extract and learn features is clear and concise.\n",
    "\n",
    "2. **Image Processing and Bounding Box Prediction**: The process of image segmentation into grids and the prediction of bounding boxes is well-described. This helps readers understand the basic mechanism behind face detection models.\n",
    "\n",
    "3. **Augmentation**: The emphasis on dataset diversity and augmentation techniques is crucial. This section highlights the importance of training models on varied data to ensure accuracy and generalization.\n",
    "\n",
    "4. **Performance Evaluation Metrics**: The explanation of key metrics like accuracy, precision, recall, and IOU is informative and helps in understanding model evaluation.\n",
    "\n",
    "### Face Verification\n",
    "\n",
    "1. **Face Encoding**: The description of face encoding using neural network architectures and the creation of embedding vectors is thorough and informative.\n",
    "\n",
    "2. **The Triplet Loss and Online Triplet Loss**: These sections provide a deep dive into the training processes, explaining the concepts clearly and providing valuable insights into the model's learning strategy.\n",
    "\n",
    "3. **Additive Angular Margin Loss**: This subsection offers an advanced perspective on how to improve the discriminative power of face recognition models. The explanation is technical but accessible.\n",
    "\n",
    "4. **Figures and Diagrams**: The included figures add significant value, helping to visualize complex concepts. They are well-integrated into the text.\n",
    "\n",
    "5. **Technical Depth**: The level of detail is appropriate for an audience with a background in machine learning or computer vision.\n",
    "\n",
    "### Suggestions for Improvement:\n",
    "\n",
    "1. **Contextual Introduction**: A brief introduction at the beginning of each subsection would set the stage for the reader, especially for those less familiar with these topics.\n",
    "\n",
    "2. **Balanced Approach**: While the technical details are comprehensive, including some real-world applications or case studies could provide context and show the practical implications of these technologies.\n",
    "\n",
    "3. **Limitations and Challenges**: Discussing potential limitations or challenges in face detection and verification technologies would provide a more balanced view.\n",
    "\n",
    "4. **Future Directions**: A section on future research trends or emerging technologies in face detection and verification would be a valuable addition.\n",
    "\n",
    "Overall, your report sections are well-constructed, informative, and reflect a deep understanding of the subject matter. Including the suggested improvements could make it even more comprehensive and engaging for the reader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```latex\n",
    "\n",
    "\\section{Preliminaries}\n",
    "\\subsection{Face Detection}\n",
    "\\subsubsection{Neural Network Architectures}\n",
    "Face Detection, as the initial phase in Face Recognition, demands the use of sophisticated neural network models, with Convolutional Neural Networks (CNNs) being the most prominent. These networks are designed to effectively recognize and localize human faces within digital images, even under varying environmental and conditional challenges. The architecture of CNNs, particularly their convolutional layers, is adept at extracting and learning spatial hierarchies of features from images, making them ideal for detecting facial features in diverse settings.\n",
    "\n",
    "\\subsubsection{Image Processing and Bounding Box Prediction}\n",
    "The face detection process begins by segmenting the input image into multiple smaller grids. Each grid cell independently predicts the likelihood of a face's presence by generating potential bounding boxes. These bounding boxes, typically rectangular, are defined by coordinates that outline the estimated location and size of faces in the image. Alongside these coordinates, each cell computes an objectness score, assessing the probability that the bounding box indeed contains a face.\n",
    "\n",
    "\\subsubsection{Augmentation}\n",
    "The key factor in achieving effective face recognition is training the model on rich and diverse datasets, including many facial images, varying in size, pose, expression sensitivity and lighting conditions. Such training ensures the model can generalize well and detect faces accurately in the diversity of real-life situations. Data augmentation techniques, such as cropping, rotating, scaling, adjusting lighting, blurring, color contrast, adding noise, etc. can be used to perform this expansion of diversity.\n",
    "\n",
    "\\subsubsection{Performance Evaluation Metrics}\n",
    "The performance of a face detection model is evaluated using several key metrics: accuracy, precision, recall, and Intersection Over Union (IOU). Accuracy provides an overall measure of the model's correct detections. Precision focuses on the proportion of true positive detections among all positive identifications made by the model, while recall measures how many actual faces are correctly identified. IOU offers a geometric assessment of the overlap between predicted bounding boxes and actual face locations, indicating the model’s accuracy in localizing faces. \n",
    "\n",
    "% [width=0.45\\textwidth] 1 nửa trang ngang\n",
    "\\begin{figure}\n",
    "    \\centering    \n",
    "    \\includegraphics[width=0.45\\textwidth]{Image/OtherPart/PRELIMINARIES_FaceDetection_IOUMetric.PNG}\n",
    "    \\caption{IOU Metric - Face Detection}\n",
    "    \\label{fig:IOU}\n",
    "\\end{figure}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***5. MODELLING***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Face Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 Yolov5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```latex\n",
    "\\section{Modelling}\n",
    "---\n",
    "\n",
    "\\subsection{Face Detection}\n",
    "\\subsubsection{YOLOv5 with YOLOv5s}\n",
    "In this section, we focus on deploying the YOLOv5s model, specifically tuned for the task of face detection. YOLO (You Only Look Once) is a state-of-the-art, real-time object detection system, and YOLOv8 is the latest in this series. The 's' in YOLOv5s stands for 'small', indicating that this model is the smallest and fastest variant, making it ideal for applications that require real-time processing with limited computational resources.\n",
    "\n",
    "\\paragraph{Model Architecture} The YOLOv5 architectural model is designed for efficiency and speed. It uses the CSPDarknet53 backbone, a modification of the Darknet configuration that was used in previous versions. New configuration of SPPF and CSP-PAN is used for the Neck part. YOLOv5 uses the output of YOLOv3 for the Head section.\n",
    "\n",
    "\\begin{enumerate}\n",
    "    \\item \\textbf{Backbone:} YOLOv5 employs a CSP-Darknet53 backbone, which is an enhanced version of the Darknet53 architecture used in YOLOv3. The CSP (Cross Stage Partial Network) strategy is used here to enable the flow of information to deep layers and reduce redundant gradients.\n",
    "    \\begin{figure}[h]\n",
    "        \\centering\n",
    "        \\includegraphics[width=0.4\\textwidth]{Image/Yolov5/Yolov5_Backbone_CSPDarknet53.PNG}\n",
    "        \\caption{Backbone CSPDarknet53 of Yolov5}\n",
    "        \\label{fig:yolov5}\n",
    "    \\end{figure}\n",
    "    \\item \\textbf{Neck:} The neck of the model is designed to construct feature pyramids that aid in detecting objects of various sizes and scales. YOLOv5 incorporates a modified Path Aggregation Network (PANet) with the CSP strategy and a variant of the Spatial Pyramid Pooling (SPP) block called SPPF for faster processing.\n",
    "    \n",
    "    \\begin{figure}[h]\n",
    "        \\centering\n",
    "        \\includegraphics[width=0.4\\textwidth]{Image/Yolov5/Yolov5_Neck_SPPF.PNG}\n",
    "        \\caption{SPPF module architecture}\n",
    "        \\label{fig:yolov5}\n",
    "    \\end{figure}\n",
    "    \n",
    "    \\item \\textbf{Head:} The head of YOLOv5 is identical to that used in YOLOv3 and YOLOv4. It performs the final prediction, generating bounding box coordinates, objectness scores, and class predictions.\n",
    "\\end{enumerate}\n",
    "\n",
    "\\paragraph{Input Preprocessing} For face detection, the input images are first resized to a standard dimension of 640x640 pixels. YOLO labels.txt format (x\\_center, y\\_center, width, height), normalized by image dimensions.\n",
    "\n",
    "\\paragraph{Detection Mechanism} YOLOv5 divides the input image into a grid, and each grid cell predicts bounding boxes, objectness scores indicating the likelihood of an object's presence, and class probabilities. For face detection, the model is trained to identify and localize faces within these bounding boxes.\n",
    "\n",
    "\n",
    "\\begin{figure}[h]\n",
    "    \\centering   \\includegraphics[width=0.4\\textwidth]{Image/Yolov5/Yolov5_DetectionMechanism_GRID.png}\n",
    "    \\caption{Detection Mechanism of Yolov5}\n",
    "    \\label{fig:yolov5}\n",
    "\\end{figure}\n",
    "\n",
    "\\paragraph{Model Performance} YOLOv5s demonstrates high accuracy and speed in face detection. Experimentally, we have only trained Yolov5 on the WIDER set with 12880 training images and 3226 validation images on 1 epoch then the detection ability of the model with actual images has been quite good. After that, we continue to train the model for additional 100 epochs. The Yolov5s model's very small size, only about 14-15MB makes it suitable for deployment in mobile applications and embedded systems.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your \"Modelling\" section for YOLOv5s in the context of face detection is well-structured and provides a thorough overview of the model's architecture and functionality. Here are some detailed observations and suggestions for enhancement:\n",
    "\n",
    "1. **Introduction to YOLOv5s**: The introduction succinctly explains the relevance and advantages of YOLOv5s, particularly its suitability for real-time processing in resource-constrained environments.\n",
    "\n",
    "2. **Model Architecture**:\n",
    "   - The breakdown into Backbone, Neck, and Head provides a clear understanding of the model's structure.\n",
    "   - The explanation of the CSPDarknet53 backbone, SPPF, and CSP-PAN offers insight into the improvements and efficiencies in YOLOv5.\n",
    "   - The figure included for the SPPF module is a great visual aid. Ensure that this image is sourced or created in compliance with copyright and citation requirements.\n",
    "\n",
    "3. **Input Preprocessing**: The information on image resizing and labeling format is essential for understanding how the model handles inputs, which is particularly useful for practical applications.\n",
    "\n",
    "4. **Detection Mechanism**: Describing how the model divides the image into a grid and makes predictions is key to understanding its operation. It might be beneficial to include a visual representation of this process.\n",
    "\n",
    "5. **Model Performance**: Discussing the model's performance using the WIDER dataset gives a good indication of its effectiveness. It would be beneficial to include specific metrics (like accuracy, speed, etc.) to quantify performance.\n",
    "\n",
    "6. **Suggestions for Improvement**:\n",
    "   - **Additional Visual Aids**: Diagrams or figures illustrating the overall architecture, the grid-based detection process, and example outputs with bounding boxes would enhance comprehension.\n",
    "   - **Comparison with Other Models**: Briefly comparing YOLOv5s with other models (like YOLOv3, YOLOv4, or non-YOLO architectures) could provide context regarding its improvements and advantages.\n",
    "   - **Hyperparameter Details**: If applicable, mentioning key hyperparameters used during training could be insightful for a technical audience.\n",
    "   - **Challenges and Limitations**: Discussing any challenges faced or limitations of the model, especially in specific scenarios, would provide a balanced view.\n",
    "   - **Future Work or Enhancements**: Suggestions for future improvements or potential research directions could be a valuable addition.\n",
    "\n",
    "Overall, the section is informative and well-crafted, suitable for an audience with a background in machine learning or computer vision. The suggested enhancements could make it even more comprehensive and insightful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phần \"Neck\" trong kiến trúc của YOLOv5 là một thành phần quan trọng, nằm giữa Backbone (cột sống) và Head (đầu) của mô hình. Mục đích chính của Neck là xử lý và kết hợp các đặc trưng (features) được trích xuất từ Backbone để chuẩn bị cho các dự đoán cuối cùng được thực hiện bởi Head. Trong YOLOv5, Neck đóng vai trò quan trọng trong việc cải thiện hiệu suất phát hiện đối tượng bằng cách kết hợp thông tin từ nhiều quy mô khác nhau. Dưới đây là một số điểm chính về Neck trong YOLOv5:\n",
    "\n",
    "1. **Path Aggregation Network (PANet)**: YOLOv5 sử dụng một phiên bản của PANet trong kiến trúc của nó. PANet giúp cải thiện luồng thông tin giữa các lớp và tăng cường sự kết nối giữa các đặc trưng ở các cấp độ sâu khác nhau của mô hình. Điều này giúp mô hình phát hiện đối tượng tốt hơn, đặc biệt là khi chúng xuất hiện ở các kích thước và tỷ lệ khác nhau.\n",
    "\n",
    "2. **Spatial Pyramid Pooling (SPP)** hoặc **SPPF (SPP-Fast)**: SPP là một khối mạng nơ-ron được sử dụng để xử lý đầu vào ở nhiều tỷ lệ không gian khác nhau, làm cho mô hình linh hoạt hơn với kích thước đầu vào. Trong YOLOv5, một phiên bản tối ưu hóa của SPP, gọi là SPPF, được sử dụng. SPPF giúp giảm thiểu độ trễ và tăng tốc độ xử lý, đặc biệt hữu ích cho các ứng dụng thời gian thực.\n",
    "\n",
    "3. **Cross Stage Partial Networks (CSP)**: YOLOv5 cũng tích hợp phong cách CSP vào cả Backbone và Neck. CSP giúp giảm bớt sự phức tạp và cải thiện hiệu suất bằng cách phân chia các đặc trưng và sau đó kết hợp chúng trở lại ở các giai đoạn khác nhau của mô hình.\n",
    "\n",
    "Tóm lại, Neck trong YOLOv5 đóng vai trò quan trọng trong việc kết hợp và tinh chỉnh các đặc trưng từ Backbone trước khi chúng được đưa đến Head để thực hiện phát hiện đối tượng. Các công nghệ như PANet, SPPF, và CSP trong Neck giúp tăng cường khả năng nhận diện đối tượng của mô hình ở nhiều quy mô và điều kiện khác nhau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 ADNet with STARLoss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```latex\n",
    "\\subsubsection{ADNet with STAR Loss}\n",
    "\\textbf{ADNet: Leveraging Error-Bias Towards Normal Direction in Face Alignment}\n",
    "\\textbf{STAR Loss: Reducing Semantic Ambiguity in Facial Landmark Detection}\n",
    "\n",
    "\\paragraph{ADNet with ADL and AAM}\n",
    "In our research on face alignment and facial feature detection, we research the ADNet network developed by Yangyu Huang, Hao Yang, Chong Li, Jongyoo Kim, and Fangyun Wei from Microsoft Research Asia. This network solves the problem of skew in the error distribution of facial feature points. ADNet uses Anisotropic Direction Loss (ADL) applies strong constraint in the normal direction of the facial boundaries and Anisotropic Attention Module (AAM) focuses on local point and edge regions providing loose constraint in the tangential direction for coordinate and heatmap regression. ADL applies strong constraint in the normal direction of the facial boundaries, while AAM . These two methods support each other in learning facial structure and texture details. ADNet has shown excellent results on datasets such as 300W, WFLW and COFW.\n",
    "\n",
    "\\begin{figure}[h]\n",
    "    \\centering\n",
    "    \\includegraphics[width=0.45\\textwidth]{Image/ADNet/ADNet_Framework.png}\n",
    "    \\caption{The Framework of ADNet}\n",
    "    \\label{fig:flowproblem}\n",
    "\\end{figure}\n",
    "\n",
    "\n",
    "\n",
    "\\paragraph{STAR Loss: Reducing Semantic Ambiguity in Facial Landmark Detection}\n",
    "STAR Loss (Self-adaptive Ambiguity\n",
    "Reduction) or self-adaptive anisotropic direction loss, reduces ambiguity in facial point detection by measuring the anisotropy of the predicted distribution, conceptualized by Zhenglin Chu, Huaxia Li, Hong Liu, Nanyang Wang, Gang Yu and Rongrong Ji. This method uses PCA for predictive analysis, determining the direction and magnitude of semantic ambiguity. STAR Loss reduces prediction error towards ambiguity, thereby improving model accuracy and stability, especially effective on the COFW, 300W and WFLW datasets.\n",
    "\\begin{figure}[h]\n",
    "    \\centering\n",
    "    \\includegraphics[width=0.45\\textwidth]{Image/ADNet/ADNet_STARLoss.png}\n",
    "    \\caption{Self-adapTive Ambiguity Reduction (STAR) loss}\n",
    "    \\label{fig:flowproblem}\n",
    "\\end{figure}\n",
    "\\paragraph{Implementation Details}\n",
    "We retrained ADNet from scratch using the WFLF (Wider Facial Landmarks in the Wild) dataset, featuring 10,000 images (7,500 for training, 2,500 for testing) with 98 landmarks each. Conducted on Google Colab's T4 GPU, each training epoch took around one hour. Remarkably, the average STAR Loss dropped from 3 to below 0.5 in just the initial epoch. After that, we have evaluated the model with a mini set of actual images right after the initial epoch, and the results were very satisfactory with quite standard landmarks. In the following epochs, the average STAR loss continues to decrease. Due to limitations of the T4 GPU colab, we train many times, each time training 3 epochs in 3 hours. NME is the main index we use for the Evaluation process.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***6. RESULTS***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Evaluation Methods\n",
    "\\subsection{Evaluation Methods}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```latex\n",
    "\\section{Result}\n",
    "\n",
    "\\subsection{Evaluation Methods}\n",
    "\\subsubsection{Face Detection}\n",
    "To evaluate our model's proficiency in face detection, we utilized a set of 1000 images from the CELEBA dataset, which was not included in our training data to ensure unbiased testing. Our assessment focuses on three primary metrics: precision, recall, and mean Intersection over Union (IOU), with a threshold set at 0.5. These metrics are crucial as precision measures the accuracy of the detected faces against the total detected objects, recall evaluates the model's ability to detect all actual faces, and mean IOU assesses the accuracy of the bounding box placement. Additionally, we use Normalized Mean Error (NME) metrics to evaluate model performance in face alignment, NME focuses on the precision of key facial landmarks against ground truth annotations to ensure accurate and reliable alignment. \n",
    "\n",
    "\\subsubsection{Face Verification}\n",
    "We evaluated our model using the standardized Labeled Face in the Wild dataset. Concretely, we follow the dataset's instruction to use a 10-fold cross validation using pre-splitted set. We also report the model Validation Rate (True Positive Rate, TPR) with given False Acceptance Rate (False Positive Rate, FAR) of [0.1, 0.01, 0.001] respectively. These two metrics specify how often the model is able to verify an user given a threshold satisfying a pre-defined FAR.\n",
    "\n",
    "Furthermore, we also collected 60 images of 12 different people and report the TPR given the aforementioned FAR.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The description of the YOLOv5s model for face detection in your text is accurate and aligns well with the typical features and applications of the YOLO (You Only Look Once) framework. Let's break down the key points for clarity:\n",
    "\n",
    "1. **Grid-based Detection**: YOLO models, including YOLOv5s, divide the input image into a grid. Each cell in this grid is responsible for predicting bounding boxes and confidence scores (objectness scores) for objects that the center of the box falls into that cell. This is a standard approach in YOLO models and is known for its efficiency in detecting objects (or faces, in this case).\n",
    "\n",
    "2. **Objectness Scores and Class Probabilities**: Each grid cell predicts not only the bounding boxes but also objectness scores, which indicate the likelihood of an object's presence in the box. Since YOLOv5s can be used for multiple object detection tasks, it generally also predicts class probabilities. However, in the context of face detection, the model might be specifically trained to recognize and classify faces only.\n",
    "\n",
    "3. **Model Training on WIDER Face Dataset**: The WIDER Face Dataset is a widely used benchmark for face detection. Training YOLOv5s on this dataset, consisting of a diverse range of faces in various scenarios, would contribute to the robustness of the model in real-world applications.\n",
    "\n",
    "4. **Performance and Application**: YOLOv5, especially the 's' (small) variant, is known for its balance between speed and accuracy, making it a suitable choice for real-time applications, including mobile and embedded systems. The fact that your experiments show good detection capabilities with actual images further confirms the model's practical effectiveness.\n",
    "\n",
    "5. **Model Size**: The 's' in YOLOv5s typically indicates a smaller, more efficient version of the model, optimized for speed and size, which makes it particularly suitable for environments where computational resources are limited, like mobile devices or embedded systems.\n",
    "\n",
    "Overall, your description of YOLOv5s for face detection accurately reflects its mechanism, performance, and typical use cases in the field of computer vision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Performance Evaluation\n",
    "\\subsection{Performance}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
